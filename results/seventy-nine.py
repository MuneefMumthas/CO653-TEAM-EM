# -*- coding: utf-8 -*-
"""CO653-TEAM-EM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MuneefMumthas/CO653-TEAM-EM/blob/main/CO653-TEAM-EM.ipynb

CO653 - Learning Machines and Intelligent Agents

Team mates:

Muneef - 22206529

Entwan - 22135347
"""

#Importing

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.utils import resample
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/MyDrive/Loan_train.csv')
test = pd.read_csv('/content/drive/MyDrive/Loan_test.csv')

train = pd.DataFrame(train)
test = pd.DataFrame(test)

train.head()

train.shape

test.shape

train.dtypes

test.dtypes

"""###Checking the missing values"""

# Calculate the percentage of missing values per column
missing_percentage = train.isnull().mean() * 100

# Display columns with more than 0% missing values
print(missing_percentage[missing_percentage > 0])

# Calculate the percentage of missing values per column
missing_percentage = test.isnull().mean() * 100

# Display columns with more than 0% missing values
print(missing_percentage[missing_percentage > 0])

missing = train[train.isnull().any(axis=1)]
missing

def handle_missing_values(data, threshold=0.5):
    """
    Handles missing values in a dataset:
    - Drops columns with more than a specified threshold of missing values.
    - Fills remaining missing values with median for numeric columns and mode for categorical columns.

    Parameters:
        data (pd.DataFrame): The input dataset.
        threshold (float): The proportion of missing values above which columns are dropped (default 0.5).

    Returns:
        pd.DataFrame: The cleaned dataset.
    """
    # Step 2: Fill numeric columns with median
    numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_columns:
        if data[col].isnull().sum() > 0:  # Only fill if there are missing values
            data[col] = data[col].fillna(data[col].median())

    # Step 3: Fill categorical columns with mode
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns
    for col in categorical_columns:
        if data[col].isnull().sum() > 0:  # Only fill if there are missing values
            data[col] = data[col].fillna(data[col].mode()[0])  # Use the most frequent value

    return data

# Example usage
train = handle_missing_values(train, threshold=0.5)

# Check the result
print(train.isnull().sum())

# Example usage
test = handle_missing_values(test, threshold=0.5)

# Check the result
print(train.isnull().sum())

train.head(10)

train = train.drop("Loan_ID", axis = 1)

test = test.drop("Loan_ID", axis = 1)

train['TotalIncome'] = train['ApplicantIncome'] + train['CoapplicantIncome']
test['TotalIncome'] = test['ApplicantIncome'] + test['CoapplicantIncome']

train['Loan_Income_Ratio'] = train['LoanAmount'] / (train['TotalIncome'] + 1)
test['Loan_Income_Ratio'] = test['LoanAmount'] / (test['TotalIncome'] + 1)

train['Credit_History'] = train['Credit_History'].fillna(0)
test['Credit_History'] = test['Credit_History'].fillna(0)

train.head()

a = train['Property_Area'].unique()
print(sorted(a))

unique = train_encoded['Dependents'].unique()
print(sorted(unique))

unique = train_encoded['Education'].unique()
print(sorted(unique))

!pip install category_encoders

import category_encoders as ce

encoder = ce.MEstimateEncoder(cols=['Gender', 'Married', 'Property_Area', 'Education', 'Self_Employed'], m=5)

# Encode features only
train_features_encoded = encoder.fit_transform(train.drop(columns=['Loan_Status']), train['Loan_Status'])

# Add target column back
train_encoded = train_features_encoded.copy()
train_encoded['Loan_Status'] = train['Loan_Status'].values
# Transform test features (without target variable)

test_encoded = encoder.transform(test)

train_encoded.head()

from sklearn.preprocessing import MinMaxScaler
# Define the columns to scale
columns_to_scale = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'TotalIncome', 'Loan_Amount_Term']
# Initialize Min-Max Scaler
scaler = MinMaxScaler()

# Fit and transform the selected columns
train_encoded[columns_to_scale] = scaler.fit_transform(train_encoded[columns_to_scale])
test_encoded[columns_to_scale] = scaler.transform(test_encoded[columns_to_scale])  # Use the same scaler for test data

from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity

# Fit and transform the 'Dependents' column
dependents_encoded = encoder.fit_transform(train_encoded[['Dependents']])

# Convert to DataFrame with meaningful column names
dependents_df = pd.DataFrame(dependents_encoded, columns=encoder.get_feature_names_out(['Dependents']))

# Concatenate the encoded columns back to the original dataset
train = pd.concat([train_encoded.drop(columns=['Dependents']), dependents_df], axis=1)

# Apply the same transformation to test data
dependents_encoded_test = encoder.transform(test_encoded[['Dependents']])
dependents_df_test = pd.DataFrame(dependents_encoded_test, columns=encoder.get_feature_names_out(['Dependents']))
test = pd.concat([test_encoded.drop(columns=['Dependents']), dependents_df_test], axis=1)

from sklearn.preprocessing import LabelEncoder

# Initialise the encoder
label_encoder = LabelEncoder()

# Fit and transform Loan_Status
train['Loan_Status'] = label_encoder.fit_transform(train['Loan_Status'])

train.head()

test.head()

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["CoapplicantIncome"])

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["LoanAmount"])

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["ApplicantIncome"])

# Plot categorical feature distributions

plt.figure(figsize=(8, 5))
sns.countplot(y=train["Loan_Status"], order=train["Loan_Status"].value_counts().index)
plt.title(f"Distribution of Loan_Status")
plt.xlabel("Count")
plt.ylabel("Loan_Status")
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(train.corr(), annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""####Pairplot"""

sns.pairplot(train, hue="Loan_Status")
plt.show()

"""In my experience:
When I upsample then split into train test, there is a chance of getting overfit because some of the test values are already seen.

But in this we already have the seperate test data we can upsample here.
"""

# Separate majority and minority classes
df_majority = train[train["Loan_Status"] == 1]
df_minority = train[train["Loan_Status"] == 0]

# Upsample minority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,  # Sample with replacement
                                 n_samples=len(df_majority),  # Match majority class count
                                 random_state=42)  # Ensure reproducibility

# Combine majority class with upsampled minority class
train = pd.concat([df_majority, df_minority_upsampled])

# Shuffle the dataset
train = train.sample(frac=1, random_state=42).reset_index(drop=True)

# Check class distribution after upsampling
print(train["Loan_Status"].value_counts())


# Check class distribution after downsampling
print(train["Loan_Status"].value_counts())

train.head()

plt.figure(figsize=(8, 5))
sns.countplot(y=train["Loan_Status"], order=train["Loan_Status"].value_counts().index)
plt.title(f"Distribution of Loan_Status in upsampled")
plt.xlabel("Count")
plt.ylabel("Loan_Status")
plt.show()

# Assume df is your dataset
X = train.drop(columns=["Loan_Status"])  # Features
y = train["Loan_Status"]  # Target variable

# Split dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Split dataset (80% training, 20% testing)


# Display the shapes of the training and test sets
(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train.head()

from keras.callbacks import ModelCheckpoint, EarlyStopping

mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)

early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)

from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import BatchNormalization, Dropout
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Define Learning Rate Scheduler
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',  # Reduce LR when validation loss stops improving
    factor=0.5,          # Reduce LR by a factor of 0.5
    patience=3,          # Wait for 3 epochs before reducing LR
    min_lr=0.00001       # Minimum LR to prevent over-reduction
)

X_train.shape[1]

from tensorflow.keras import backend as K
K.clear_session()

model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    Dropout(0.4),

    tf.keras.layers.Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    tf.keras.layers.Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    tf.keras.layers.Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),

    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),  # Reduce LR for better convergence
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train the Model with LR Scheduler
history = model.fit(
    X_train, y_train,
    epochs=50,
    validation_data=(X_test, y_test),
    callbacks=[mc, early_stopping, lr_scheduler],  # Add lr_scheduler here
    verbose=1
)

loss = history.history['loss']
val_loss = history.history['val_loss']
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(1, len(loss) + 1)

# Plotting both loss and accuracy in one figure
plt.figure(figsize=(12, 6))

# First subplot for loss
plt.subplot(1, 2, 1)
plt.plot(epochs, loss, label='Training Loss', color='blue')
plt.plot(epochs, val_loss, label='Validation Loss', color='green')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Second subplot for accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy, label='Training Accuracy', color='red')
plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='purple')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

from tensorflow.keras.models import load_model

model = load_model("best_model.h5")

# Get model predictions
predictions = model.predict(test, verbose=1)

# Load test dataset
test_data = pd.read_csv("/content/drive/MyDrive/Loan_test.csv")

# Store Loan_ID separately before dropping
loan_ids = test_data["Loan_ID"].values

# Convert predictions to 'Y' and 'N'
predicted_classes = np.where(predictions > 0.5, "Y", "N")

# Ensure Loan_IDs and predictions have the same length
assert len(loan_ids) == len(predicted_classes), "Mismatch in Loan_ID and predictions length!"

# Create a DataFrame for submission
submission = pd.DataFrame({
    "Loan_ID": loan_ids,  # Adding back Loan_ID
    "Loan_Status": predicted_classes.flatten()
})

# Save to CSV
submission.to_csv("/content/drive/MyDrive/submission.csv", index=False)

print("Predictions saved as submission.csv")

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)
print(feature_importance.sort_values(ascending=False))