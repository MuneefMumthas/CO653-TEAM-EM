# -*- coding: utf-8 -*-
"""CO653-TEAM-EM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MuneefMumthas/CO653-TEAM-EM/blob/main/CO653-TEAM-EM.ipynb

CO653 - Learning Machines and Intelligent Agents

Team mates:

Muneef - 22206529

Entwan - 22135347
"""

#Importing

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.utils import resample
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/MyDrive/Loan_train.csv')
test = pd.read_csv('/content/drive/MyDrive/Loan_test.csv')

train = pd.DataFrame(train)
test = pd.DataFrame(test)

train.head()

train.shape

test.shape

train.dtypes

test.dtypes

# Calculate the percentage of missing values per column
missing_percentage = train.isnull().mean() * 100

# Display columns with more than 0% missing values
print(missing_percentage[missing_percentage > 0])

# Calculate the percentage of missing values per column
missing_percentage = test.isnull().mean() * 100

# Display columns with more than 0% missing values
print(missing_percentage[missing_percentage > 0])

missing = train[train.isnull().any(axis=1)]
missing

def handle_missing_values(data, threshold=0.5):
    """
    Handles missing values in a dataset:
    - Drops columns with more than a specified threshold of missing values.
    - Fills remaining missing values with median for numeric columns and mode for categorical columns.

    Parameters:
        data (pd.DataFrame): The input dataset.
        threshold (float): The proportion of missing values above which columns are dropped (default 0.5).

    Returns:
        pd.DataFrame: The cleaned dataset.
    """
    # Step 2: Fill numeric columns with median
    numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_columns:
        if data[col].isnull().sum() > 0:  # Only fill if there are missing values
            data[col] = data[col].fillna(data[col].median())

    # Step 3: Fill categorical columns with mode
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns
    for col in categorical_columns:
        if data[col].isnull().sum() > 0:  # Only fill if there are missing values
            data[col] = data[col].fillna(data[col].mode()[0])  # Use the most frequent value

    return data

# Example usage
train = handle_missing_values(train, threshold=0.5)

# Check the result
print(train.isnull().sum())

# Example usage
test = handle_missing_values(test, threshold=0.5)

# Check the result
print(train.isnull().sum())

train.head(10)

#train = train.drop("Loan_ID", axis = 1)

#test = test.drop("Loan_ID", axis = 1)

# Get categorical columns present in the current DataFrame
categorical_columns = train.select_dtypes(include=['object', 'category']).columns

label_encoder = LabelEncoder()
# Dictionary to store mappings
label_mappings_train = {}


for column in categorical_columns:
    train[column] = label_encoder.fit_transform(train[column])
    label_mappings_train[column] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Get categorical columns present in the current DataFrame
categorical_columns = test.select_dtypes(include=['object', 'category']).columns


label_mappings_test = {}

for column in categorical_columns:
    test[column] = label_encoder.fit_transform(test[column])
    label_mappings_test[column] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Print the mappings
for col, mapping in label_mappings_train.items():
    print(f"Column: {col}")
    print(mapping)
    print()

train.head()

# Print the mappings
for col, mapping in label_mappings_test.items():
    print(f"Column: {col}")
    print(mapping)
    print()

train.head(15)

test.head()

train["ApplicantIncome"].describe()

train["LoanAmount"].describe()

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["ApplicantIncome"])

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["CoapplicantIncome"])

'ApplicantIncome	CoapplicantIncome	LoanAmount'

sns.boxplot(train["LoanAmount"])

unique = train["Loan_Amount_Term"].unique()
unique

# Define the numerical columns that need scaling
high_num_cols = ["ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term"]

# Create a copy of the DataFrame (optional, to avoid modifying the original)
train_scaled = train.copy()
test_scaled = test.copy()

# Apply StandardScaler only to the selected numerical columns
scaler = StandardScaler()
train_scaled[high_num_cols] = scaler.fit_transform(train_scaled[high_num_cols])
test_scaled[high_num_cols] = scaler.fit_transform(test_scaled[high_num_cols])

# Now df_scaled has the scaled numerical features while categorical ones remain unchanged

train_scaled.head()

# Plot categorical feature distributions
for col in categorical_columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(y=train_scaled[col], order=train[col].value_counts().index)
    plt.title(f"Distribution of {col}")
    plt.xlabel("Count")
    plt.ylabel(col)
    plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(train_scaled.corr(), annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

sns.pairplot(train_scaled, hue="Loan_Status")
plt.show()

# Plot categorical feature distributions

plt.figure(figsize=(8, 5))
sns.countplot(y=train_scaled["Loan_Status"], order=train["Loan_Status"].value_counts().index)
plt.title(f"Distribution of Loan_Status")
plt.xlabel("Count")
plt.ylabel("Loan_Status")
plt.show()

"""In my experience:
When I upsample then split into train test, there is a chance of getting overfit because some of the test values are already seen.

But in this we already have the seperate test data we can upsample here.
"""

# Separate majority and minority classes
df_majority = train_scaled[train_scaled["Loan_Status"] == 1]
df_minority = train_scaled[train_scaled["Loan_Status"] == 0]

# Upsample minority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,  # Sample with replacement
                                 n_samples=len(df_majority),  # Match majority class count
                                 random_state=42)  # Ensure reproducibility

# Combine majority class with upsampled minority class
train_upsample = pd.concat([df_majority, df_minority_upsampled])

# Shuffle the dataset
train_upsample = train_upsample.sample(frac=1, random_state=42).reset_index(drop=True)

# Check class distribution after upsampling
print(train_upsample["Loan_Status"].value_counts())

# Downsample majority class
df_majority_downsampled = resample(df_majority,
                                   replace=False,  # No replacement (randomly remove samples)
                                   n_samples=len(df_minority),  # Match minority class count
                                   random_state=42)  # Ensure reproducibility

# Combine minority class with downsampled majority class
train_downsample = pd.concat([df_majority_downsampled, df_minority])

# Shuffle the dataset
train_downsample = train_downsample.sample(frac=1, random_state=42).reset_index(drop=True)

# Check class distribution after downsampling
print(train_downsample["Loan_Status"].value_counts())

plt.figure(figsize=(8, 5))
sns.countplot(y=train_downsample["Loan_Status"], order=train_downsample["Loan_Status"].value_counts().index)
plt.title(f"Distribution of Loan_Status in downsampled")
plt.xlabel("Count")
plt.ylabel("Loan_Status")
plt.show()

plt.figure(figsize=(8, 5))
sns.countplot(y=train_upsample["Loan_Status"], order=train_upsample["Loan_Status"].value_counts().index)
plt.title(f"Distribution of Loan_Status in upsampled")
plt.xlabel("Count")
plt.ylabel("Loan_Status")
plt.show()

train_downsample.head()

train_upsample.head()

# Assume df is your dataset
X_up = train_upsample.drop(columns=["Loan_Status"])  # Features
y_up = train_upsample["Loan_Status"]  # Target variable

X_down = train_downsample.drop(columns=["Loan_Status"])  # Features
y_down = train_downsample["Loan_Status"]  # Target variable

# Split dataset (80% training, 20% testing)
X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(X_up, y_up, test_size=0.2, random_state=42)
# Split dataset (80% training, 20% testing)
X_train_down, X_test_down, y_train_down, y_test_down = train_test_split(X_down, y_down, test_size=0.2, random_state=42)


# Display the shapes of the training and test sets
(X_train_up.shape, X_test_up.shape, y_train_up.shape, y_test_up.shape)

(X_train_down.shape, X_test_down.shape, y_train_down.shape, y_test_down.shape)

from keras.callbacks import ModelCheckpoint, EarlyStopping

mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)

early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=25, verbose=1)

#Creating a model for csv dataset
model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_up.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['accuracy'])

# Training the model
history = model_1.fit(X_train_up, y_train_up,
                      epochs=50,
                      validation_data=(X_test_up, y_test_up),
                      callbacks=[mc, early_stopping],
                      verbose=1)

loss = history.history['loss']
val_loss = history.history['val_loss']
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(1, len(loss) + 1)

# Plotting both loss and accuracy in one figure
plt.figure(figsize=(12, 6))

# First subplot for loss
plt.subplot(1, 2, 1)
plt.plot(epochs, loss, label='Training Loss', color='blue')
plt.plot(epochs, val_loss, label='Validation Loss', color='green')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Second subplot for accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy, label='Training Accuracy', color='red')
plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='purple')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

#Creating a model for csv dataset
model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train_down.shape[1],)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['accuracy'])

# Training the model
history = model_2.fit(X_train_down, y_train_down,
                      epochs=50,
                      validation_data=(X_test_down, y_test_down),
                      callbacks=[mc, early_stopping],
                      verbose=1)

loss = history.history['loss']
val_loss = history.history['val_loss']
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(1, len(loss) + 1)

# Plotting both loss and accuracy in one figure
plt.figure(figsize=(12, 6))

# First subplot for loss
plt.subplot(1, 2, 1)
plt.plot(epochs, loss, label='Training Loss', color='blue')
plt.plot(epochs, val_loss, label='Validation Loss', color='green')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Second subplot for accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy, label='Training Accuracy', color='red')
plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='purple')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

from tensorflow.keras.models import load_model

model = load_model("best_model.h5")

test_scaled.shape

test_scaled.head()

test_scaled.head()

# Get model predictions
predictions = model.predict(test_scaled, verbose=1)

# Reverse transform Loan_ID using the stored label mappings
loan_id_mapping = {v: k for k, v in label_mappings_test["Loan_ID"].items()}  # Reverse the dictionary
loan_ids = [loan_id_mapping[i] for i in test_scaled["Loan_ID"].values]  # Map back to original Loan_IDs

# Convert predictions to 'Y' and 'N'
predicted_classes = np.where(predictions > 0.5, "Y", "N")

# Ensure Loan_IDs and predictions have the same length
assert len(loan_ids) == len(predicted_classes), "Mismatch in Loan_ID and predictions length!"

# Create a DataFrame for submission
submission = pd.DataFrame({
    "Loan_ID": loan_ids,  # Using reversed Loan_IDs
    "Predicted_Class": predicted_classes.flatten()
})

# Save to CSV
submission.to_csv("/content/drive/MyDrive/submission1.csv", index=False)

print("Predictions saved as submission.csv")